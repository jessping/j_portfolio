{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cf2a85",
   "metadata": {},
   "source": [
    "# Deep Leaning Project - Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e55c3",
   "metadata": {},
   "source": [
    "## Project Purpose:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1d915",
   "metadata": {},
   "source": [
    "The purpose of this project is to perform image classification using a neural network. The project aims to train a model that can accurately classify images of shapes into three categories: circles, squares, and triangles. The project involves preprocessing the image data, building and training a neural network model, evaluating the model's performance, and using the trained model to make predictions on new images.\n",
    "\n",
    "The overall goal is to develop a reliable image classification system that can automatically identify and categorize different shapes based on their visual features. This type of project has various applications, such as object recognition, quality control in manufacturing, computer vision tasks, and many others where automated shape classification is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d6815",
   "metadata": {},
   "source": [
    "The technique used in this project is image classification using a neural network. Specifically, a deep learning approach is employed to train a model capable of accurately classifying images of shapes into three categories: circles, squares, and triangles.\n",
    "\n",
    "The technique used in this project leverages deep learning and neural networks to automatically learn and extract relevant features from images, enabling accurate classification of shapes. Overview of the techniques used in this project:\n",
    "\n",
    "- Data Collection and Preparation: The project starts with collecting a dataset of images representing different shapes. In this case, the dataset includes images of circles, squares, and triangles. The images are organized into separate folders based on their corresponding shape labels. The dataset is then preprocessed, including tasks such as converting images to grayscale, applying thresholding, and flattening the image arrays.\n",
    "\n",
    "\n",
    "- Neural Network Model: The neural network model used in this project is implemented using the Keras library. It is a sequential model that consists of multiple layers. The architecture of the model includes dense layers with various activation functions, dropout layers for regularization, and a final output layer with softmax activation for multi-class classification. The model's structure is designed to learn and capture the complex relationships between the input image features and the target shape labels.\n",
    "\n",
    "\n",
    "- Model Training: The model is trained using the prepared dataset. The dataset is split into training and testing sets to evaluate the model's performance. The training process involves optimizing the model's parameters by minimizing a specified loss function (in this case, categorical cross-entropy) using the Adam optimizer. Early stopping is employed as a callback to prevent overfitting and restore the best weights based on validation accuracy.\n",
    "\n",
    "\n",
    "- Evaluation and Visualization: The trained model is evaluated using the testing dataset. Performance metrics such as accuracy are computed to assess the model's accuracy in correctly classifying shapes. Additionally, visualizations, such as line plots, are generated to analyze the model's training and validation accuracy and loss over epochs.\n",
    "\n",
    "\n",
    "- Prediction on New Images: Finally, the trained model can be used to make predictions on new images. The new image is preprocessed in a similar manner as the training data, passed through the model, and the predicted shape label is obtained. This allows the model to classify unseen images based on its learned patterns and features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee15261",
   "metadata": {},
   "source": [
    "The project utilizes several libraries for various tasks as follows:\n",
    "\n",
    "- Pandas: Pandas is used for data manipulation and analysis. It provides data structures and functions to efficiently work with structured data, such as loading data into DataFrames, manipulating columns, and performing exploratory data analysis.\n",
    "\n",
    "\n",
    "- NumPy: NumPy is a fundamental library for numerical computations in Python. It provides support for multi-dimensional arrays and various mathematical operations, which are crucial for working with image data and performing calculations in the neural network model.\n",
    "\n",
    "\n",
    "- OpenCV: OpenCV (Open Source Computer Vision Library) is used for image processing tasks, such as reading and manipulating images. It provides functions for image loading, resizing, converting to grayscale, applying filters, and other image-related operations.\n",
    "\n",
    "\n",
    "- os: The os module is a built-in Python library used for interacting with the operating system. In this project, it is used to navigate and manipulate file paths, specifically for accessing and listing the image files in the dataset folders.\n",
    "\n",
    "\n",
    "- Matplotlib.pyplot: Matplotlib is a plotting library in Python. The pyplot module provides functions for creating various types of plots and visualizations. It is used in this project to plot and visualize the training and validation accuracy, loss, and other metrics.\n",
    "\n",
    "\n",
    "- Seaborn: Seaborn is a data visualization library built on top of Matplotlib. It provides a higher-level interface for creating attractive statistical visualizations. In this project, it is used to create the heatmap of the confusion matrix (heatmap() function).\n",
    "\n",
    "\n",
    "- Scikit-learn: Scikit-learn is a popular machine learning library in Python. In this project, it is used for tasks such as splitting the dataset into training and testing sets (train_test_split()) and calculating performance metrics (accuracy_score, confusion_matrix) for evaluating the model.\n",
    "\n",
    "\n",
    "- Keras: Keras is a high-level deep learning library that provides an interface to build and train neural network models. It is used in this project to define and train the image classification model with its sequential model API. It also provides various layers (Dense, Dropout) and callbacks (EarlyStopping) used in the model architecture and training process. \n",
    "\n",
    "These libraries collectively enable data processing, image manipulation, neural network modeling, model training, evaluation, and visualization, facilitating the complete workflow of the image classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing the pandas library for data handling\n",
    "import cv2  # Importing the OpenCV library for image processing\n",
    "import numpy as np  # Importing the NumPy library for array manipulation\n",
    "import os  # Importing the os module for accessing the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6beeb5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "data = {\"label\": [], \"flatten\": []}  # Dictionary to store labels and flattened images\n",
    "\n",
    "imgName = {\"circles\": [], \"triangles\": [], \"squares\": []}  # Dictionary to store file names for each shape category\n",
    "basePath = {  # Dictionary defining the file paths for each shape category\n",
    "    \"circles\": r\"D:\\My Files\\Courses\\Python\\80_ANN\\13_ANN_Classification\\shapes - Copy\\circles\",\n",
    "    \"triangles\": r\"D:\\My Files\\Courses\\Python\\80_ANN\\13_ANN_Classification\\shapes - Copy\\triangles\",\n",
    "    \"squares\": r\"D:\\My Files\\Courses\\Python\\80_ANN\\13_ANN_Classification\\shapes - Copy\\squares\"\n",
    "}\n",
    "\n",
    "for key in imgName.keys():  # Iterate over the keys of imgName dictionary\n",
    "    imgName[key].append(os.listdir(basePath[key]))  # Populate the lists with file names for each shape category\n",
    "\n",
    "print(len(imgName[\"circles\"][0]))  # Print the number of images for the \"circles\" category\n",
    "print(len(imgName[\"triangles\"][0]))  # Print the number of images for the \"triangles\" category\n",
    "print(len(imgName[\"squares\"][0]))  # Print the number of images for the \"squares\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "02507d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in imgName.keys():  # Iterate over the keys of imgName dictionary\n",
    "    for i, n in enumerate(imgName[k][0]):  # Iterate over the file names for each shape category\n",
    "        path = os.path.join(basePath[k], n)  # Get the full file path by joining the base path and file name\n",
    "        img = np.array(cv2.imread(path, cv2.IMREAD_GRAYSCALE))  # Read the image and convert it to grayscale\n",
    "\n",
    "        for r in range(0, 3):  # Rotate the image three times\n",
    "            img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)  # Rotate the image 90 degrees clockwise\n",
    "            cv2.imwrite(path + \"rotated\" + str(k) + str(i) + \".png\", img)  # Save the rotated image with a new file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bd64b707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n",
      "399\n",
      "399\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>flatten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>circles</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>circles</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>circles</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>circles</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>circles</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            flatten\n",
       "0  circles  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1  circles  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2  circles  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3  circles  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4  circles  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"label\": [], \"flatten\": []}  # Dictionary to store labels and flattened images\n",
    "\n",
    "imgName = {\"circles\": [], \"triangles\": [], \"squares\": []}  # Dictionary to store file names for each shape category\n",
    "basePath = {  # Dictionary defining the file paths for each shape category\n",
    "    \"circles\": r\"D:\\My Files\\Courses\\Python\\80_ANN\\13_ANN_Classification\\shapes - Copy\\circles\",\n",
    "    \"triangles\": r\"D:\\My Files\\Courses\\Python\\80_ANN\\13_ANN_Classification\\shapes - Copy\\triangles\",\n",
    "    \"squares\": r\"D:\\My Files\\Courses\\Python\\80_ANN\\13_ANN_Classification\\shapes - Copy\\squares\"\n",
    "}\n",
    "\n",
    "for key in imgName.keys():  # Iterate over the keys of imgName dictionary\n",
    "    imgName[key].append(os.listdir(basePath[key]))  # Populate the lists with file names for each shape category\n",
    "\n",
    "print(len(imgName[\"circles\"][0]))  # Print the number of images for the \"circles\" category\n",
    "print(len(imgName[\"triangles\"][0]))  # Print the number of images for the \"triangles\" category\n",
    "print(len(imgName[\"squares\"][0]))  # Print the number of images for the \"squares\" category\n",
    "\n",
    "for k in imgName.keys():  # Iterate over the keys of imgName dictionary\n",
    "    for n in imgName[k][0]:  # Iterate over the file names for each shape category\n",
    "        path = os.path.join(basePath[k], n)  # Get the full file path by joining the base path and file name\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  # Read the image and convert it to grayscale\n",
    "        img = np.array(img)  # Convert the image to a NumPy array\n",
    "        img = cv2.bitwise_not(img)  # Invert the image using bitwise_not\n",
    "        img[img > 0] = 1  # Threshold the image to convert it into binary values (0 or 1)\n",
    "        img = img.flatten()  # Flatten the image into a 1D array\n",
    "        data[\"flatten\"].append(img)  # Add the flattened image to the \"flatten\" list in the data dictionary\n",
    "        data[\"label\"].append(k)  # Add the corresponding label to the \"label\" list in the data dictionary\n",
    "\n",
    "data = pd.DataFrame(data)  # Convert the data dictionary into a pandas DataFrame\n",
    "data.head()  # Display the first few rows of the DataFrame\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "48a9d0c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1   2   3   4   5   6   7   8   9    ... 774 775 776 777 778 779 780  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "1   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "2   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "4   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "  781 782 783  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "4   0   0   0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = [x for x in range(0, len(data[\"flatten\"][0]))]  # Generate a list of column numbers\n",
    "df = pd.DataFrame(columns=col)  # Create an empty DataFrame with the specified columns\n",
    "\n",
    "for i in range(0, len(data[\"flatten\"])):  # Iterate over the flattened images in the data dictionary\n",
    "    df = df.append(pd.Series(data[\"flatten\"][i]), ignore_index=True)  # Append each flattened image as a new row to the DataFrame\n",
    "\n",
    "df.head()  # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5afbd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # Importing train_test_split from sklearn\n",
    "from keras.models import Sequential  # Importing the Sequential class from keras.models\n",
    "from keras.layers import Dense, Dropout  # Importing Dense and Dropout layers from keras.layers\n",
    "from keras.callbacks import EarlyStopping, History  # Importing EarlyStopping and History callbacks from keras.callbacks\n",
    "\n",
    "y = np.array(pd.get_dummies(data[\"label\"]), np.int64)  # Convert the label column to one-hot encoded array\n",
    "X = np.array(df, np.int64)  # Convert the DataFrame to a NumPy array\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  # Split the data into training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "28b42e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_79 (Dense)             (None, 2048)              1607680   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 3,977,059\n",
      "Trainable params: 3,977,059\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "48/48 [==============================] - 2s 49ms/step - loss: 1.0715 - accuracy: 0.4232 - val_loss: 0.9399 - val_accuracy: 0.5833\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.9373 - accuracy: 0.5580 - val_loss: 0.8015 - val_accuracy: 0.6458\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.7950 - accuracy: 0.6458 - val_loss: 0.9153 - val_accuracy: 0.5500\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.7118 - accuracy: 0.6865 - val_loss: 0.6107 - val_accuracy: 0.7292\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.5942 - accuracy: 0.7283 - val_loss: 0.7529 - val_accuracy: 0.7042\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.4379 - accuracy: 0.8192 - val_loss: 0.5700 - val_accuracy: 0.7625\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.3501 - accuracy: 0.8537 - val_loss: 0.6906 - val_accuracy: 0.7667\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.3351 - accuracy: 0.8683 - val_loss: 0.5484 - val_accuracy: 0.8125\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.2686 - accuracy: 0.9101 - val_loss: 0.4821 - val_accuracy: 0.8042\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.1642 - accuracy: 0.9478 - val_loss: 0.7447 - val_accuracy: 0.7708\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.2259 - accuracy: 0.9237 - val_loss: 0.4584 - val_accuracy: 0.8292\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.0997 - accuracy: 0.9603 - val_loss: 0.5553 - val_accuracy: 0.8167\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.1756 - accuracy: 0.9436 - val_loss: 0.5948 - val_accuracy: 0.8125\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.1046 - accuracy: 0.9551 - val_loss: 0.6389 - val_accuracy: 0.8417\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.0825 - accuracy: 0.9697 - val_loss: 0.6432 - val_accuracy: 0.8042\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 2s 34ms/step - loss: 0.0730 - accuracy: 0.9728 - val_loss: 0.6816 - val_accuracy: 0.7958\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.1029 - accuracy: 0.9572 - val_loss: 0.5916 - val_accuracy: 0.8167\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.0375 - accuracy: 0.9896 - val_loss: 0.9519 - val_accuracy: 0.8083\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.1276 - accuracy: 0.9603 - val_loss: 0.5144 - val_accuracy: 0.8417\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 2s 37ms/step - loss: 0.0661 - accuracy: 0.9770 - val_loss: 0.7044 - val_accuracy: 0.8250\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 2s 36ms/step - loss: 0.0825 - accuracy: 0.9781 - val_loss: 0.5440 - val_accuracy: 0.8375\n",
      "Epoch 22/100\n",
      "48/48 [==============================] - 2s 35ms/step - loss: 0.0640 - accuracy: 0.9781 - val_loss: 0.7130 - val_accuracy: 0.8125\n",
      "Epoch 23/100\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.0862 - accuracy: 0.9728 - val_loss: 0.5586 - val_accuracy: 0.8542\n",
      "Epoch 24/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0353 - accuracy: 0.9885 - val_loss: 0.8938 - val_accuracy: 0.8208\n",
      "Epoch 25/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.0578 - accuracy: 0.9770 - val_loss: 0.5889 - val_accuracy: 0.8625\n",
      "Epoch 26/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0429 - accuracy: 0.9843 - val_loss: 0.7288 - val_accuracy: 0.8542\n",
      "Epoch 27/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0525 - accuracy: 0.9885 - val_loss: 0.7215 - val_accuracy: 0.8417\n",
      "Epoch 28/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0685 - accuracy: 0.9801 - val_loss: 0.5863 - val_accuracy: 0.8083\n",
      "Epoch 29/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0485 - accuracy: 0.9885 - val_loss: 0.6284 - val_accuracy: 0.8583\n",
      "Epoch 30/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0484 - accuracy: 0.9843 - val_loss: 0.6257 - val_accuracy: 0.8542\n",
      "Epoch 31/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0196 - accuracy: 0.9906 - val_loss: 0.9855 - val_accuracy: 0.8042\n",
      "Epoch 32/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0412 - accuracy: 0.9885 - val_loss: 0.7016 - val_accuracy: 0.8625\n",
      "Epoch 33/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0238 - accuracy: 0.9948 - val_loss: 0.7145 - val_accuracy: 0.8417\n",
      "Epoch 34/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0326 - accuracy: 0.9916 - val_loss: 0.8804 - val_accuracy: 0.8500\n",
      "Epoch 35/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.0387 - accuracy: 0.9927 - val_loss: 0.9172 - val_accuracy: 0.8417\n",
      "Epoch 36/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0518 - accuracy: 0.9843 - val_loss: 0.5989 - val_accuracy: 0.8458\n",
      "Epoch 37/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0190 - accuracy: 0.9979 - val_loss: 0.9354 - val_accuracy: 0.8333\n",
      "Epoch 38/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0410 - accuracy: 0.9885 - val_loss: 0.9404 - val_accuracy: 0.8167\n",
      "Epoch 39/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0268 - accuracy: 0.9885 - val_loss: 0.6618 - val_accuracy: 0.8542\n",
      "Epoch 40/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0158 - accuracy: 0.9927 - val_loss: 0.6237 - val_accuracy: 0.8625\n",
      "Epoch 41/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.7857 - val_accuracy: 0.8542\n",
      "Epoch 42/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0061 - accuracy: 0.9979 - val_loss: 0.8459 - val_accuracy: 0.8625\n",
      "Epoch 43/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.2139 - val_accuracy: 0.8292\n",
      "Epoch 44/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 1.3056 - val_accuracy: 0.8375\n",
      "Epoch 45/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0235 - accuracy: 0.9927 - val_loss: 0.9439 - val_accuracy: 0.8375\n",
      "Epoch 46/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0735 - accuracy: 0.9843 - val_loss: 0.8264 - val_accuracy: 0.8250\n",
      "Epoch 47/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0883 - accuracy: 0.9728 - val_loss: 0.8032 - val_accuracy: 0.8292\n",
      "Epoch 48/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0615 - accuracy: 0.9801 - val_loss: 0.8302 - val_accuracy: 0.8333\n",
      "Epoch 49/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.8386 - val_accuracy: 0.8208\n",
      "Epoch 50/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0139 - accuracy: 0.9937 - val_loss: 0.8475 - val_accuracy: 0.8292\n",
      "Epoch 51/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0591 - accuracy: 0.9812 - val_loss: 0.6581 - val_accuracy: 0.8333\n",
      "Epoch 52/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0289 - accuracy: 0.9916 - val_loss: 0.7788 - val_accuracy: 0.8500\n",
      "Epoch 53/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.8734 - val_accuracy: 0.8417\n",
      "Epoch 54/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0141 - accuracy: 0.9958 - val_loss: 0.9765 - val_accuracy: 0.8458\n",
      "Epoch 55/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0623 - accuracy: 0.9843 - val_loss: 1.0140 - val_accuracy: 0.7958\n",
      "Epoch 56/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0302 - accuracy: 0.9906 - val_loss: 0.7046 - val_accuracy: 0.8583\n",
      "Epoch 57/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0188 - accuracy: 0.9958 - val_loss: 0.9314 - val_accuracy: 0.8583\n",
      "Epoch 58/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0179 - accuracy: 0.9948 - val_loss: 0.9054 - val_accuracy: 0.8500\n",
      "Epoch 59/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0326 - accuracy: 0.9906 - val_loss: 0.7856 - val_accuracy: 0.8458\n",
      "Epoch 60/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.7336 - val_accuracy: 0.8542\n",
      "Epoch 61/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0118 - accuracy: 0.9990 - val_loss: 0.6849 - val_accuracy: 0.8667\n",
      "Epoch 62/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0239 - accuracy: 0.9927 - val_loss: 0.8763 - val_accuracy: 0.8458\n",
      "Epoch 63/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0216 - accuracy: 0.9948 - val_loss: 0.8229 - val_accuracy: 0.8417\n",
      "Epoch 64/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 1.0649 - val_accuracy: 0.8417\n",
      "Epoch 65/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0264 - accuracy: 0.9948 - val_loss: 1.0525 - val_accuracy: 0.8583\n",
      "Epoch 66/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0224 - accuracy: 0.9927 - val_loss: 1.2264 - val_accuracy: 0.8292\n",
      "Epoch 67/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0593 - accuracy: 0.9843 - val_loss: 0.6554 - val_accuracy: 0.8333\n",
      "Epoch 68/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0163 - accuracy: 0.9958 - val_loss: 0.7831 - val_accuracy: 0.8458\n",
      "Epoch 69/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 1.0522 - val_accuracy: 0.8417\n",
      "Epoch 70/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 1.1396 - val_accuracy: 0.8542\n",
      "Epoch 71/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0118 - accuracy: 0.9937 - val_loss: 0.9695 - val_accuracy: 0.8625\n",
      "Epoch 72/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0376 - accuracy: 0.9885 - val_loss: 2.2011 - val_accuracy: 0.7750\n",
      "Epoch 73/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.1067 - accuracy: 0.9728 - val_loss: 0.7981 - val_accuracy: 0.8292\n",
      "Epoch 74/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0158 - accuracy: 0.9937 - val_loss: 0.8628 - val_accuracy: 0.8167\n",
      "Epoch 75/100\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.8560 - val_accuracy: 0.8375\n",
      "Epoch 76/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0422 - accuracy: 0.9896 - val_loss: 0.9012 - val_accuracy: 0.8167\n",
      "Epoch 77/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0422 - accuracy: 0.9843 - val_loss: 0.7947 - val_accuracy: 0.8375\n",
      "Epoch 78/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0114 - accuracy: 0.9958 - val_loss: 0.8967 - val_accuracy: 0.8208\n",
      "Epoch 79/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0319 - accuracy: 0.9916 - val_loss: 1.1246 - val_accuracy: 0.8167\n",
      "Epoch 80/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0588 - accuracy: 0.9875 - val_loss: 1.3702 - val_accuracy: 0.8042\n",
      "Epoch 81/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0232 - accuracy: 0.9896 - val_loss: 1.0011 - val_accuracy: 0.8625\n",
      "Epoch 82/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0186 - accuracy: 0.9916 - val_loss: 1.0367 - val_accuracy: 0.8500\n",
      "Epoch 83/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0072 - accuracy: 0.9958 - val_loss: 1.0388 - val_accuracy: 0.8542\n",
      "Epoch 84/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0201 - accuracy: 0.9916 - val_loss: 1.0322 - val_accuracy: 0.8333\n",
      "Epoch 85/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0081 - accuracy: 0.9948 - val_loss: 1.1860 - val_accuracy: 0.8500\n",
      "Epoch 86/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0186 - accuracy: 0.9958 - val_loss: 1.2354 - val_accuracy: 0.8500\n",
      "Epoch 87/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0169 - accuracy: 0.9937 - val_loss: 1.0680 - val_accuracy: 0.8375\n",
      "Epoch 88/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0253 - accuracy: 0.9927 - val_loss: 1.3903 - val_accuracy: 0.8208\n",
      "Epoch 89/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0641 - accuracy: 0.9770 - val_loss: 0.7824 - val_accuracy: 0.8542\n",
      "Epoch 90/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0609 - accuracy: 0.9875 - val_loss: 0.8983 - val_accuracy: 0.8208\n",
      "Epoch 91/100\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.0368 - accuracy: 0.9927 - val_loss: 0.8216 - val_accuracy: 0.8583\n",
      "Epoch 92/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0316 - accuracy: 0.9979 - val_loss: 1.0485 - val_accuracy: 0.8500\n",
      "Epoch 93/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 1.0212 - val_accuracy: 0.8417\n",
      "Epoch 94/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0213 - accuracy: 0.9948 - val_loss: 1.1329 - val_accuracy: 0.8250\n",
      "Epoch 95/100\n",
      "48/48 [==============================] - 1s 21ms/step - loss: 0.0185 - accuracy: 0.9896 - val_loss: 0.8301 - val_accuracy: 0.8458\n",
      "Epoch 96/100\n",
      "48/48 [==============================] - 1s 22ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.0433 - val_accuracy: 0.8625\n",
      "Epoch 97/100\n",
      "48/48 [==============================] - 1s 24ms/step - loss: 0.0200 - accuracy: 0.9937 - val_loss: 1.1353 - val_accuracy: 0.8500\n",
      "Epoch 98/100\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.0411 - val_accuracy: 0.8792\n",
      "Epoch 99/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.0160 - accuracy: 0.9979 - val_loss: 1.4060 - val_accuracy: 0.8500\n",
      "Epoch 100/100\n",
      "48/48 [==============================] - 1s 23ms/step - loss: 0.0255 - accuracy: 0.9958 - val_loss: 1.2632 - val_accuracy: 0.8458\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()  # Creating a Sequential model\n",
    "\n",
    "# Adding a Dense layer with 2048 units and ReLU activation function\n",
    "model.add(Dense(2048, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "# Adding a Dropout layer with dropout rate of 0.5\n",
    "model.add(Dropout(0.5))  \n",
    "\n",
    "model.add(Dense(1024, activation='relu'))  # Adding a Dense layer with 1024 units and ReLU activation function\n",
    "model.add(Dense(256, activation='relu'))  # Adding a Dense layer with 256 units and ReLU activation function\n",
    "model.add(Dropout(0.4))  # Adding a Dropout layer with dropout rate of 0.4\n",
    "\n",
    "model.add(Dense(32, activation='relu'))  # Adding a Dense layer with 32 units and ReLU activation function\n",
    "model.add(Dense(16, activation='relu'))  # Adding a Dense layer with 16 units and ReLU activation function\n",
    "model.add(Dropout(0.1))  # Adding a Dropout layer with dropout rate of 0.1\n",
    "\n",
    "# Adding a Dense layer with 3 units (number of classes) and softmax activation function\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Displaying the model summary\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model with categorical cross-entropy loss, Adam optimizer, and accuracy metric\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Early stopping callback to monitor validation accuracy and stop training if it doesn't improve for 40 epochs\n",
    "stop = EarlyStopping(monitor=\"val_accuracy\", mode='max', patience=40, restore_best_weights=True)\n",
    "\n",
    "# Training the model with training data, validation data, callbacks, epochs, batch size, and verbose mode\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks=[stop], epochs=100, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8e258021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history.history.keys()  # Retrieve the keys of the training history dictionary\n",
    "his = history.history  # Assign the training history dictionary to the variable 'his'\n",
    "\n",
    "accuracy = his[\"accuracy\"]  # Extract the accuracy values from the history\n",
    "val_acc = his[\"val_accuracy\"]  # Extract the validation accuracy values from the history\n",
    "loss = his[\"loss\"]  # Extract the loss values from the history\n",
    "val_loss = his[\"val_loss\"]  # Extract the validation loss values from the history\n",
    "\n",
    "plt.plot(accuracy, color=\"blue\")  # Plot the accuracy values with blue color\n",
    "plt.plot(val_acc, color=\"red\")  # Plot the validation accuracy values with red color\n",
    "plt.plot(loss, color=\"yellow\")  # Plot the loss values with yellow color\n",
    "\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b4df700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8458333333333333\n"
     ]
    }
   ],
   "source": [
    "#0 circle #1 square #2 triangle\n",
    "pred = model.predict(X_test)  # Perform predictions on the test data\n",
    "yPred = []  # Initialize an empty list to store the predicted labels\n",
    "\n",
    "for i in range(0, len(X_test)):\n",
    "    index = np.argmax(pred[i])  # Get the index of the maximum predicted value\n",
    "    \n",
    "    if index == 0:\n",
    "        yPred.append(\"circle\")\n",
    "    elif index == 1:\n",
    "        yPred.append(\"square\")\n",
    "    elif index == 2:\n",
    "        yPred.append(\"triangle\")\n",
    "\n",
    "yTrue = []  # Initialize an empty list to store the true labels\n",
    "for i in range(0, len(y_test)):\n",
    "    index = np.argmax(y_test[i])  # Get the index of the maximum true value\n",
    "    \n",
    "    if index == 0:\n",
    "        yTrue.append(\"circle\")\n",
    "    elif index == 1:\n",
    "        yTrue.append(\"square\")\n",
    "    elif index == 2:\n",
    "        yTrue.append(\"triangle\")\n",
    "\n",
    "score = pd.DataFrame(columns=[\"yTrue\", \"yPred\"])  # Create a DataFrame to store the true and predicted labels\n",
    "score[\"yTrue\"] = yTrue  # Assign the true labels to the \"yTrue\" column\n",
    "score[\"yPred\"] = yPred  # Assign the predicted labels to the \"yPred\" column\n",
    "\n",
    "from sklearn.metrics import accuracy_score  # Import the accuracy_score function from sklearn.metrics\n",
    "accuracy = accuracy_score(yPred, yTrue)  # Calculate the accuracy score\n",
    "print(accuracy)  # Print the accuracy score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c4421952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'true')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix  # Import the confusion_matrix function from sklearn.metrics\n",
    "import seaborn as sns  # Import the seaborn library as sns\n",
    "\n",
    "con = confusion_matrix(yPred, yTrue)  # Calculate the confusion matrix\n",
    "sns.heatmap(con, annot=True, cbar=False)  # Create a heatmap of the confusion matrix with annotations and without colorbar\n",
    "plt.xlabel('Predicted')  # Set the x-label of the plot as \"Predicted\"\n",
    "plt.ylabel('True')  # Set the y-label of the plot as \"True\"\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "295b9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triangle\n"
     ]
    }
   ],
   "source": [
    "path = r\"D:\\My Files\\Courses\\Python\\80_ANN\\13_ANN_Classification\\predictT.png\"  # Specify the path to the image to be predicted\n",
    "img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  # Read the image in grayscale\n",
    "img = np.array(img)  # Convert the image to a numpy array\n",
    "img = cv2.bitwise_not(img)  # Invert the image using bitwise_not\n",
    "img[img > 0] = 1  # Threshold the image to binary values (0 or 1)\n",
    "img = img.flatten()  # Flatten the image into a 1D array\n",
    "img.shape  # Print the shape of the flattened image\n",
    "\n",
    "predSingle = model.predict(img.reshape(1, 784))  # Perform prediction on the single image\n",
    "index = np.argmax(predSingle)  # Get the index of the maximum predicted value\n",
    "\n",
    "if index == 0:\n",
    "    print(\"circle\")\n",
    "elif index == 1:\n",
    "    print(\"square\")\n",
    "elif index == 2:\n",
    "    print(\"triangle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d033723",
   "metadata": {},
   "source": [
    "### **Project Insights:**\n",
    "\n",
    "1. Accuracy and Performance: The project provides insights into the accuracy and performance of the image classification model. By evaluating metrics such as accuracy, loss, and validation accuracy over epochs, you can assess how well the model is learning and generalizing to unseen data. This information helps in understanding the effectiveness of the model in classifying shapes.\n",
    "\n",
    "\n",
    "2. Model Architecture and Hyperparameters: The project allows for insights into the model architecture and hyperparameters that contribute to the classification performance. By examining the model summary, you can see the number of layers, their sizes, and activation functions. Adjusting the architecture, such as adding or removing layers, changing activation functions, or tuning hyperparameters, can potentially improve the model's accuracy.\n",
    "\n",
    "\n",
    "3. Confusion Matrix and Misclassifications: The confusion matrix provides insights into the types of misclassifications made by the model. By analyzing the confusion matrix, you can identify which shapes are commonly misclassified and understand the patterns of errors. This information can guide further improvements in the model, such as collecting more training data for specific classes or adjusting the model architecture to address specific challenges.\n",
    "\n",
    "\n",
    "4. Visualization of Training Process: The line plots of accuracy, validation accuracy, and loss over epochs provide insights into the training process. By analyzing these plots, you can observe how the model's performance changes during training, identify underfitting or overfitting issues, and determine the optimal number of epochs. This knowledge helps in optimizing the training process and improving the model's performance.\n",
    "\n",
    "\n",
    "5. Predicting New Images: The ability to predict the shape of new images using the trained model provides insights into the model's real-world applicability. By applying the model to unseen images and observing the predictions, you can assess how well the model generalizes to different shapes and identify any limitations or areas for improvement.\n",
    "\n",
    "Overall, the valuable insights from this project include understanding the accuracy and performance of the model, gaining insights into the model architecture and hyperparameters, analyzing misclassifications through the confusion matrix, visualizing the training process, and evaluating the model's performance on new images. These insights can guide further iterations, improvements, and applications of the image classification system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517da4a9",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf5b27",
   "metadata": {},
   "source": [
    "In conclusion, this image classification project demonstrates the effectiveness of deep learning techniques in accurately classifying shapes. By training a neural network model on a dataset of circles, squares, and triangles, valuable insights have been gained regarding the model's performance, architecture, and real-world applicability.\n",
    "\n",
    "The trained model achieved a commendable level of accuracy in classifying shapes, as evidenced by the evaluation metrics and performance visualization. It effectively learned the complex relationships between input image features and shape labels, providing a reliable classification system. The model's architecture, comprising multiple layers with appropriate activation functions and regularization techniques, contributed to its ability to learn and generalize from the training data.\n",
    "\n",
    "The project's analysis of the confusion matrix shed light on specific misclassifications, allowing for further investigation and potential improvements. Understanding the patterns of errors can guide the collection of additional data or adjustments to the model's architecture to address specific challenges.The visualization of the training process provided insights into the model's learning dynamics, helping to identify optimal training epochs and detect any overfitting or underfitting issues. This information can guide future iterations and refinements of the model to enhance its performance.\n",
    "\n",
    "Moreover, the successful prediction of new images using the trained model demonstrated its real-world applicability. By applying the model to unseen images, it showcased the potential for automated shape classification in various practical scenarios.\n",
    "\n",
    "Overall, this project highlights the power of deep learning and neural networks in image classification tasks. It provides a foundation for further exploration and refinement, opening doors to broader applications of shape classification and paving the way for advancements in computer vision and automated image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496ed5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
